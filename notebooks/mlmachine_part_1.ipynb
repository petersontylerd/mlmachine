{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__mlmachine - Clean ML Experiments, Elegant EDA & Pandas Pipelines__\n",
    "<br><br>\n",
    "Check out the [GitHub repository](https://github.com/petersontylerd/mlmachine).\n",
    "<br><br>\n",
    "\n",
    "1. [What is mlmachine?](#What-is-mlmachine?)\n",
    "1. [The Machine Class - A Hub With Many Spokes](#The-Machine-Class-A-Hub-With-Many-Spokes)\n",
    "    1. [Data Intake](#Data-Intake)\n",
    "    1. [mlm dtypes: Adding Feature Meaning to Pandas dtypes](#mlm-dtypes:-Adding-Feature-Meaning-to-Pandas-dtypes)\n",
    "1. [Because EDA is Tedious and Takes Forever](#Because-EDA-is-Tedious-and-Takes-Forever)\n",
    "    1. [Categorical Feature Panels](#Categorical-Feature-Panels)\n",
    "        1. [Effortless Extension to Multi-class Problems](#Effortless-Extension-to-Multi-class-Problems)\n",
    "    1. [Continuous Feature Panels](#Continuous-Feature-Panels)\n",
    "        1. [Another Effortless Extension to Multi-class Problems](#Another-Effortless-Extension-to-Multi-class-Problems)\n",
    "1. [Pandas In / Pandas Out Pipelines](#Pandas-In-/-Pandas-Out-Pipelines)\n",
    "    1. [Scikit-learn Dismantles Pandas DataFrames](#Scikit-learn-Dismantles-Pandas-DataFrames)\n",
    "    1. [Transformers, Now with DataFrames](#Transformers,-Now-with-DataFrames)\n",
    "    1. [PandasFeatureUnion & DataFrameSelector - Intuitive, Familiar, Flexible](#PandasFeatureUnion-&-DataFrameSelector-Intuitive,-Familiar,-Flexible)\n",
    "        1. [Vanilla FeatureUnion](#Vanilla-FeatureUnion)\n",
    "        1. [PandasFeatureUnion & DataFrameSelector](#PandasFeatureUnion-&-DataFrameSelector)\n",
    "            1. [Basic Example](#Basic-Example)\n",
    "            1. [Less Basic Example](#Less-Basic-Example)\n",
    "        1. [Updating mlm_dtypes](#Updating-mlm_dtypes)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# What is mlmachine?\n",
    "---\n",
    "<br><br>\n",
    "Notebooks often serve as scratch paper for data scientists. Machine learning experiments tend to become messy, disjointed series of hard-coded blocks. Even if time is taken to write general purpose functions, those functions live isolated, uselessly locked away from new projects.\n",
    "<br><br>\n",
    "\n",
    "mlmachine is a Python package that facilitates clean and organized notebook-based machine learning experimentation and accomplishes many key aspects of the experimentation life cycle.\n",
    "<br><br>\n",
    "\n",
    "The central hub of mlmachine is the `Machine()` class. A `Machine()` object retains the dataset, target data and feature meta data. More importantly, a `Machine()` object has numerous built-in methods for quickly executing key parts of the machine learning experiment workflow.\n",
    "<br><br>\n",
    "\n",
    "Here are a few of the core areas of mlmachine's functionality that we will explore in detail:\n",
    "1. __Data intake & `mlm_dtype` identification__\n",
    "2. __Exploratory data analysis__ - here is an example of just how easy it is to create a panel of visualizations and data summaries for a feature:\n",
    "\n",
    "```python\n",
    "# create single categorical feature EDA panel\n",
    "mlmachine_demo.eda_cat_target_cat_feat(\n",
    "    feature=\"Embarked\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")\n",
    "```\n",
    "<br><br>\n",
    "![alt text](images/p1_eda_panel.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "3. __Pandas-friendly transformers and pipelines__ - see how simply wrapping the mlmachine utility `PandasTransformer()` around `OneHotEncoder()` maintains our `DataFrame`:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p1_pandastransformer.jpeg \"PandasTransformer\")\n",
    "<br><br>\n",
    "\n",
    "mlmachine contains an immense amount of functionality aimed at saving time and improving model performance, all while keeping our keeping our workflow clean and organized.\n",
    "<br><br>\n",
    "\n",
    "Let's get started.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'What-is-mlmachine?'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Machine Class - A Hub With Many Spokes\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'The-Machine-Class-A-Hub-With-Many-Spokes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Data Intake\n",
    "---\n",
    "<br><br>\n",
    "We begin by instantiating a `Machine()` object:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Data-Intake'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.655204Z",
     "start_time": "2020-04-03T04:09:47.819726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\mlmachine-env\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'SHAP'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-37f05bd07650>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import mlmachine tools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmlmachine\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmlmachine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env\\lib\\site-packages\\mlmachine\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"0.0.35\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmachine\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMachine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_df_compile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env\\lib\\site-packages\\mlmachine\\machine.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mMachine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \"\"\"\n\u001b[0;32m     46\u001b[0m     \u001b[0mDocumentation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env\\lib\\site-packages\\mlmachine\\machine.py\u001b[0m in \u001b[0;36mMachine\u001b[1;34m()\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mregression_panel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     )\n\u001b[1;32m---> 99\u001b[1;33m     from .model.explain.visualize import (\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mmulti_shap_value_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mmulti_shap_viz_tree\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\mlmachine-env\\lib\\site-packages\\mlmachine\\model\\explain\\visualize.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mprettierplot\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mSHAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'SHAP'"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import mlmachine tools\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "# use titanic() function to create DataFrames for training and validation datasets\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "# instantiate a Machine object\n",
    "mlmachine_titanic = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\",\"Cabin\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings={\"Pclass\": [1, 2, 3]},\n",
    "    is_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's unpack what we just did. Using the canonical Titanic dataset, we instantiate a `Machine()` object, called `mlmachine_titanic`, by:\n",
    "- Passing in the full dataset as a `DataFrame`\n",
    "- Specifying the column that contains the target variable\n",
    "- Specifying the supervised learning task as a classification task\n",
    "<br><br>\n",
    "\n",
    "The most basic purpose of `mlmachine_titanic` is to maintain our dataset of observations and our target values. Our dataset is stored as a `DataFrame` and can be accessed by calling `mlmachine_titanic.data`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.657200Z",
     "start_time": "2020-04-03T04:09:47.819Z"
    }
   },
   "outputs": [],
   "source": [
    "# review dataset\n",
    "mlmachine_titanic.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Our target variable, stored as a named Pandas `Series`, can be accessed just as easily by calling `mlmachine_titanic.target`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.658199Z",
     "start_time": "2020-04-03T04:09:47.821Z"
    }
   },
   "outputs": [],
   "source": [
    "# review target values\n",
    "mlmachine_titanic.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "The target values will be automatically label encoded, if needed.\n",
    "<br><br>\n",
    "\n",
    "We also passed several lists containing feature names to parameters such as `identify_as_continuous` and `identify_as_nominal`. Let's get into the purpose of these parameters.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## mlm dtypes: Adding Feature Meaning to Pandas dtypes\n",
    "---\n",
    "<br><br>\n",
    "Pandas dtypes describe the values contained within a column, but have no regard for what the values actually _mean_. Nominal categories, ordinal categories, continuous numbers, counts…it's often impossible to make these distinctions from Pandas dtypes alone. \n",
    "<br><br>\n",
    "\n",
    "In the mlmachine ecosystem, these distinctions are referred to as `mlm_dtypes`. mlmachine catalogs and, most importantly, updates `mlm_dtypes` as the dataset evolves through feature engineering.\n",
    "<br><br>\n",
    "\n",
    "Our dataset, stored as `mlmachine_titanic.data`, has a metadata attribute called `mlm_dtypes`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'mlm-dtypes:-Adding-Feature-Meaning-to-Pandas-dtypes'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.659199Z",
     "start_time": "2020-04-03T04:09:47.824Z"
    }
   },
   "outputs": [],
   "source": [
    "# review dataset's mlm dtypes\n",
    "mlmachine_titanic.data.mlm_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "This dictionary is a cornerstone of the mlmachine workflow and permeates throughout all of the package's functionality. Notice the dictionary's keys. Per the guidance we provided when instantiating the `Machine()` object, `mlm_dtypes` stores the mlm dtype for each feature.\n",
    "<br><br>\n",
    "\n",
    "The dictionary keys make it particularly easy to reference all features of a certain type without having to type out feature names. The practical benefit of this is obvious, especially if we consider datasets larger than this tiny Titanic dataset. \n",
    "<br><br>\n",
    "\n",
    "In this article, we're going to leverage this efficiency in two key areas:\n",
    "- Exploratory data analysis\n",
    "- Transformations and pipelines\n",
    "<br><br>\n",
    "\n",
    "Let's put `mlm_dtypes` to use as we introduce mlmachine's exploratory data analysis capabilities.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Because EDA is Tedious and Takes Forever\n",
    "---\n",
    "<br><br>\n",
    "We're all guilty of performing a cursory EDA, if any at all (\"let's just get to the model training!\"). Even with all of the great Python visualization libraries out there, EDA can take a considerable amount of setup. Coding those same, slightly modified functions for the hundredth time is something we all do. And remembering which visualization types work best for which feature types and feature/target type combination is not easy.\n",
    "<br><br>\n",
    "\n",
    "Skipping EDA is absolutely a mistake, so a portion of mlmachine's functionality is dedicated to quickly making panels that are as beneficial as they are good looking.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Because-EDA-is-Tedious-and-Takes-Forever'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Categorical Feature Panels\n",
    "---\n",
    "<br><br>\n",
    "We saw a teaser EDA panel in the introduction for the nominal category feature \"Embarked\". Let's go beyond this example by using our `mlm_dtypes` dictionary to quickly generate panels for all of our categorical features in `mlmachine_titanic.data`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Categorical-Feature-Panels'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.660199Z",
     "start_time": "2020-04-03T04:09:47.826Z"
    }
   },
   "outputs": [],
   "source": [
    "# create EDA panel for all \"category\" features\n",
    "for feature in mlmachine_titanic.data.mlm_dtypes[\"category\"]:\n",
    "    mlmachine_titanic.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        legend_labels=[\"Died\",\"Survived\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`eda_cat_target_cat_feat()` generates an EDA panel for a categorical or count feature in the context of a categorical target. There are three summary tables at the top:\n",
    "1. Feature Summary - Simple count of each level in the category, along with the percent of values each level constitutes in the feature.\n",
    "2. Feature vs. target summary - Count of each level in the category, grouped by the classes in the target\n",
    "3. Target proportion - The percent of values of a particular feature level, grouped by the classes in the target.\n",
    "<br><br>\n",
    "\n",
    "The panel includes three visualization. From left to right:\n",
    "1. Tree map of the categorical feature.\n",
    "2. Bar chart of the categorical feature, faceted by the target.\n",
    "3. 100% horizontal stacked bar chart, faceted by the target.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Effortless Extension to Multi-class Problems\n",
    "---\n",
    "<br><br>\n",
    "Now let's use `eda_cat_target_cat_feat()` to generate a panel for a multi-class example. We'll use the Scikit-learn wine dataset to visualize a numeric feature called \"alcalinity_of_ash\" that has been segmented into 5 bins, effectively making it a categorical column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Effortless-Extension-to-Multi-class-Problems'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.661199Z",
     "start_time": "2020-04-03T04:09:47.829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sklearn.datasets as datasets\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# load wine dataset into DataFrame\n",
    "dataset = datasets.load_wine()\n",
    "dataset = pd.merge(\n",
    "            pd.DataFrame(dataset.data, columns=dataset.feature_names),\n",
    "            pd.Series(dataset.target, name=\"Class label\"),\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "        )\n",
    "\n",
    "# instantiate a Machine object\n",
    "mlmachine_wine = mlm.Machine(\n",
    "    data=dataset,\n",
    "    identify_as_continuous=list(dataset.columns[:-1]),\n",
    "    target=\"Class label\",\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "# instantiated object for creating 5 binned features\n",
    "binner = KBinsDiscretizer(n_bins=5, encode=\"ordinal\")\n",
    "\n",
    "# transform continuous values of \"alcalinity_of_ash\" into 5 equal bins\n",
    "mlmachine_wine.data[\"alcalinity_of_ash_binned\"] = binner.fit_transform(\n",
    "    mlmachine_wine.data[\"alcalinity_of_ash\"].values.reshape(-1, 1)\n",
    ")\n",
    "\n",
    "# create single categorical feature EDA panel\n",
    "mlmachine_wine.eda_cat_target_cat_feat(\n",
    "    feature=\"alcalinity_of_ash_binned\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Each component of the panel adapts accordingly to the multi-class problem in this dataset.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Continuous Feature Panels\n",
    "---\n",
    "<br><br>\n",
    "Now let's see what mlmachine can do with numeric features:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Continuous-Feature-Panels'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.662200Z",
     "start_time": "2020-04-03T04:09:47.832Z"
    }
   },
   "outputs": [],
   "source": [
    "# create single numeric feature EDA panel\n",
    "mlmachine_titanic.eda_cat_target_num_feat(\n",
    "    feature=\"Age\",\n",
    "    legend_labels=[\"Died\",\"Survived\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "`eda_cat_target_num_feat()` is a method that generates a panel for a numeric features in the context of a categorical target. At the top, we display three Pandas `DataFrames`:\n",
    "1. Feature Summary - All of the summary statistics we get by executing the standard `df.describe()` command, plus \"percent missing\", \"skew\" and \"kurtosis\".\n",
    "1. Feature vs. target summary - Count, proportion, mean and standard deviation of the numeric feature, grouped by the different classes in the target.\n",
    "3. Statistical test - If the target column only has two classes, this reports the result of a z-test (or t-test, in the case of small samples) and the associated p-value.\n",
    "<br><br>\n",
    "\n",
    "Below the summary tables is a panel containing four visualizations. From left to right, starting in the top left corner:\n",
    "1. Univariate distribution plot of the numeric feature.\n",
    "2. QQ plot of the numeric feature.\n",
    "3. Bivariate distribution plot of the numeric feature, faceted by the target.\n",
    "4. Horizontal box plot, faceted by the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Another Effortless Extension to Multi-class Problems\n",
    "---\n",
    "<br><br>\n",
    "`eda_cat_target_num_feat()` also adapts to multi-class problems easily. Let's look at another quick example:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Another-Effortless-Extension-to-Multi-class-Problems'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.664200Z",
     "start_time": "2020-04-03T04:09:47.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# create single numeric feature EDA panel\n",
    "mlmachine_wine.eda_cat_target_num_feat(\n",
    "    feature=\"alcalinity_of_ash\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "We again create this panel using the Scikit-learn wine dataset with the same minimal code. Notice the changes:\n",
    "1. The \"Feature vs. target summary\" table expands to reflect all three classes.\n",
    "2. The faceted plots expand to visualize all three classes.\n",
    "3. The x-axis and y-axis tick labels are decimals rather than whole numbers. This modification happens dynamically under the hood based on the scale of the data being visualized. Less time formatting, more time exploring.\n",
    "<br><br>\n",
    "\n",
    "mlmachine introduces an immense amount of simplicity and dynamism to EDA. Now let's see how mlmachine  facilitates a Pandas-friendly ML experimentation workflow.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Pandas In / Pandas Out Pipelines\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Pandas-In-/-Pandas-Out-Pipelines'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scikit-learn Dismantles Pandas DataFrames\n",
    "---\n",
    "<br><br>\n",
    "A major drawback of putting a `DataFrame` through an Scikit-learn transformer is the loss of the `DataFrame` wrapper around the underlying NumPy array. The issue is particularly pronounced with transformers like `PolynomialFeatures()`:\n",
    "<br><br>\n",
    "![alt text](images/p2_pandastransformer.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "And if we think we've outsmarted this transformer by accessing the `poly.get_feature_names()` attribute, we'll be woefully disappointed once we see the output:\n",
    "<br><br>\n",
    "![alt text](images/p2_features.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "Not helpful.\n",
    "<br><br>\n",
    "\n",
    "Due to this, we lose the ability to:\n",
    "- Easily perform EDA on the transformed dataset\n",
    "- Evaluate feature importance after training a model\n",
    "- Use model explainability methods such as SHAP or LIME\n",
    "- Merely identity which columns are which\n",
    "<br><br>\n",
    "\n",
    "Of course we could feed the NumPy array back into a `DataFrame`, and do whatever is needed to get the columns to match up, but…what a chore.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Scikit-learn-Dismantles-Pandas-DataFrames'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transformers, Now with DataFrames\n",
    "---\n",
    "<br><br>\n",
    "mlmachine leverages a class called `PandasTransformer()` to ensure that if a `DataFrame` passes into a transformer, a `DataFrame` comes out on the other side.\n",
    "<br><br>\n",
    "\n",
    "All we have to do was wrap `PolynomialFeatures()` with `PandasTransformer()` and we get a `DataFrame` with meaningful column names on the other side:\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p2_pandastransformer_2.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "It's that easy.\n",
    "<br><br>\n",
    "\n",
    "Now that we've seen how to preserve our `DataFrame` when executing a single transformation, let's build on this with Scikit-learn's `Pipeline()` and `FeatureUnion()` functionality to perform multiple actions on multiple sets of features in one shot.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Transformers,-Now-with-DataFrames'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## PandasFeatureUnion & DataFrameSelector - Intuitive, Familiar, Flexible\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PandasFeatureUnion-&-DataFrameSelector-Intuitive,-Familiar,-Flexible'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Vanilla FeatureUnion\n",
    "---\n",
    "<br><br>\n",
    "Scikit-learn includes a class called `FeatureUnion()`. To quote the documentation, `FeatureUnion()` \"Concatenates results of multiple transformer objects…This is useful to combine several feature extraction mechanisms into a single transformer.\"\n",
    "<br><br>\n",
    "\n",
    "This is a great tool for applying different data processing actions to different features. For example, we may want to mean impute continuous features and mode impute categorical features:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p2_featureunion.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "Unfortunately, `FeatureUnion()` also suffers from the same disadvantage as other transformers - it returns a NumPy array. This is where `PandasFeatureUnion()` comes to the rescue.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Vanilla FeatureUnion'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### PandasFeatureUnion & DataFrameSelector\n",
    "---\n",
    "<br><br>\n",
    "Just like we need `PandasTransformer()` to retain the `DataFrame()` post-transformation, we need `PandasFeatureUnion()` to maintain the final `DataFrame` post-concatenation.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'PandasFeatureUnion-&-DataFrameSelector'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Basic Example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Basic-Example'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.665201Z",
     "start_time": "2020-04-03T04:09:47.840Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# import mlmachine tools\n",
    "import mlmachine as mlm\n",
    "from mlmachine.data import titanic\n",
    "\n",
    "from mlmachine.features.preprocessing import (\n",
    "    DataFrameSelector,\n",
    "    PandasTransformer,\n",
    "    PandasFeatureUnion,\n",
    ")\n",
    "\n",
    "# use titanic() function to create DataFrames for training and validation datasets\n",
    "df_train, df_valid = titanic()\n",
    "\n",
    "# instantiate a Machine object\n",
    "mlmachine_titanic = mlm.Machine(\n",
    "    data=df_train,\n",
    "    target=\"Survived\",\n",
    "    remove_features=[\"PassengerId\",\"Ticket\",\"Name\"],\n",
    "    identify_as_continuous=[\"Age\",\"Fare\"],\n",
    "    identify_as_count=[\"Parch\",\"SibSp\"],\n",
    "    identify_as_nominal=[\"Embarked\",\"Cabin\"],\n",
    "    identify_as_ordinal=[\"Pclass\"],\n",
    "    ordinal_encodings = {\"Pclass\": [1, 2, 3]},\n",
    "    is_classification=True,\n",
    ")\n",
    "\n",
    "# create imputation PandasFeatureUnion pipeline\n",
    "impute_pipe = PandasFeatureUnion([\n",
    "    (\"age\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Age\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"embarked\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Embarked\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"most_frequent\"))\n",
    "    )),\n",
    "    (\"cabin\", make_pipeline(\n",
    "        DataFrameSelector(include_columns=[\"Cabin\"]),\n",
    "        PandasTransformer(SimpleImputer(strategy=\"constant\", fill_value=\"X\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(exclude_columns=[\"Age\",\"Embarked\",\"Cabin\"])\n",
    "    )),\n",
    "])\n",
    "\n",
    "# execute imputation\n",
    "mlmachine_titanic.data = impute_pipe.fit_transform(mlmachine_titanic.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.666200Z",
     "start_time": "2020-04-03T04:09:47.842Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "We start fresh here by again instantiating a `Machine()` object called `mlmachine_titanic`. Then we use mlmachine's `PandasFeatureUnion()` class to create a `DataFrame`-friendly, `FeatureUnion`-style pipeline called `impute_pipe`. Specifically, we perform three different types of imputations on three different columns.\n",
    "- Impute \"Age\" with the mean\n",
    "- Impute \"Embarked\" with the mode\n",
    "- Impute \"Cabin\" with a constant value (X).\n",
    "<br><br>\n",
    "\n",
    "A keen observer will notice the presence of another class - `DataFrameSelector()` - within each pipeline. This class is an essential element of the `PandasFeatureUnion()` workflow, and serves different purposes depending on how it's used. On lines 30, 34 and 38, `DataFrameSelector()` is used to select the column for that particular branch of the union. The columns are selected by name using the `include_columns` parameter.\n",
    "<br><br>\n",
    "\n",
    "On line 42, we do something a bit different. Since `FeatureUnion()` operations, by design, act on specific columns and concatenate the results, we would be left with only the transformed columns without further intervention.\n",
    "<br><br>\n",
    "\n",
    "That is why `DataFrameSelector()` has the flexibility to select all columns except those specified. By way of the `exclude_columns` parameter, we select all features except for the features we imputed. This ensures we keep our full dataset.\n",
    "<br><br>\n",
    "\n",
    "Now that we have filled in our nulls values, let's advance to a slightly more complicated preprocessing step using the `PandasFeatureUnion()` workflow. And if there is any question about the purpose of `mlm_dtypes`, that will become even clearer right now.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Less Basic Example\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Less-Basic-Example'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.667201Z",
     "start_time": "2020-04-03T04:09:47.844Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# create encoding PandasFeatureUnion pipeline\n",
    "encode_pipe = PandasFeatureUnion([\n",
    "    (\"nominal\", make_pipeline(\n",
    "        DataFrameSelector(\n",
    "            include_mlm_dtypes=[\"nominal\"],\n",
    "            exclude_columns=[\"Cabin\"]\n",
    "        ),\n",
    "        PandasTransformer(OneHotEncoder())\n",
    "    )),\n",
    "    (\"ordinal\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"ordinal\"]),\n",
    "        PandasTransformer(\n",
    "            OrdinalEncoder(categories=list(mlmachine_titanic.ordinal_encodings.values()))\n",
    "        )\n",
    "    )),\n",
    "    (\"bin\", make_pipeline(\n",
    "        DataFrameSelector(include_mlm_dtypes=[\"continuous\"]),\n",
    "        PandasTransformer(KBinsDiscretizer(encode=\"ordinal\"))\n",
    "    )),\n",
    "    (\"diff\", make_pipeline(\n",
    "        DataFrameSelector(\n",
    "            exclude_columns=[i for i in mlmachine_titanic.data.mlm_dtypes[\"nominal\"] if i != \"Cabin\"]\\\n",
    "            + list(mlmachine_titanic.ordinal_encodings.keys())\n",
    "        )\n",
    "    )),\n",
    "])\n",
    "\n",
    "# execute encoding\n",
    "mlmachine_titanic.data = encode_pipe.fit_transform(mlmachine_titanic.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.668201Z",
     "start_time": "2020-04-03T04:09:47.845Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "We now have a DataFrame with encoded columns, all clearly named. Let's take this `PandasFeatureUnion` branch by branch:\n",
    "- \"nominal\" pipeline - Here we see the flexibility of `DataFrameSelector()`. First, we select all nominal columns by passing `[\"nominal\"]` to the `include_mlm_dtypes` parameter. `DataFrameSelector()` directly references `mlm_dtypes` to make column selections. Second, we exclude \"Cabin\" (also a nominal feature) by passing the feature name to the `exclude_columns` parameter. `DataFrameSelector()` reconciles our include/exclude specifications by selecting all nominal columns except \"Cabin\". Lastly, we pass our selected columns to `OneHotEncoder()`, wrapped in `PandasTransformer()`.\n",
    "<br><br>\n",
    "\n",
    "- \"ordinal\" pipeline - We again us the `DataFrameSelector()` parameter `include_mlm_dtypes`, this time to select all ordinal columns. Then we pass our result to `OrdinalEncoder()`, wrapped in `PandasTransformer()`. We also provide encoding instructions. When we instantiated our `Machine()` object, we passed in a dictionary called `ordinal_encodings`, which `mlmachine_titanic` stores as an attribute. We wrap this dictionary's values in a list and pass it to the `OrdinalEncoder()` parameter `categories`. This will ensure the desired hierarchy is enforced during encoding.\n",
    "<br><br>\n",
    "\n",
    "- \"bin\" pipeline - We again use `include_mlm_dtypes` to select all continuous features and pass the result to `KBinsDiscretizer()`, wrapped in `PandasTransformer()`.\n",
    "<br><br>\n",
    "\n",
    "- \"diff\" pipeline - The last step is to recombine any features that would otherwise be lost in the union operation, and drop any features we no longer need. We perform a list comprehension on the `mlm_dtypes` attribute to remove \"Cabin\", and append the keys of `mlmachine_titanic.ordinal_encodings` to the result. This will ensure that the original nominal and ordinal features are not in the transformed dataset, but that we retain \"Cabin\". Notice that we do not exclude the continuous columns, despite the fact we transformed these features with `KBinsDiscretizer()`. The reason is simple - we want to keep the original continuous columns in our dataset.\n",
    "\n",
    "We call `encode_pipe` with the familiar `fit_transform()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Updating mlm_dtypes\n",
    "---\n",
    "<br><br>\n",
    "Since we have new features in the dataset, it is best practice to follow `fit_transform()` with `update_mlm_dtypes()`:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.669198Z",
     "start_time": "2020-04-03T04:09:47.847Z"
    }
   },
   "outputs": [],
   "source": [
    "# update mlm_dtypes\n",
    "mlmachine_titanic.update_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.671201Z",
     "start_time": "2020-04-03T04:09:47.849Z"
    }
   },
   "outputs": [],
   "source": [
    "mlmachine_titanic.data.mlm_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "As alluded to earlier in this article, we can update `mlm_dtypes` to reflect the current state of the data attribute. Let see a before-and-after to clearly see how `mlm_dtypes` dictionary changed:\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "![alt text](images/p1_mlm_dtypes.jpeg \"EDA Panel\")\n",
    "<br><br>\n",
    "\n",
    "Our updated `mlm_dtypes` dictionary is on the right. We see that the nominal columns \"Embarked\" and \"Sex\" are absent, and in their place we see names for dummy columns, such as \"Embarked_C\" and \"Sex_male\", resulting from `PandasTransformer(OneHotEncoder())`. Also notice that the \"nominal\" key  still contains the \"Cabin\" feature, which we chose to leave unprocessed at this point. \n",
    "<br><br>\n",
    "\n",
    "The \"ordinal\" key contains our binned versions of \"Age\" and \"Fare\", as well as \"Pclass\", which has been named in a way that clearly indicates the type of encoding applied. \n",
    "<br><br>\n",
    "\n",
    "Whenever we modify `data`, we simply call `update_mlm_dtypes()`, and `mlm_dtypes` will automatically update to reflect the current state of the dataset. The only real effort is to identify the mlm dtype for each feature from the outset, which is something we should do every time anyway.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Updating-mlm_dtypes'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Let's bring things to a conclusion by leveraging the `mlm_dtypes` dictionary to quickly perform a little EDA on our new features. This time, we'll cycle through all of our ordinal features:\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'In-Closing'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T04:09:50.671201Z",
     "start_time": "2020-04-03T04:09:47.852Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create EDA panel for all \"ordinal\" features\n",
    "for feature in mlmachine_titanic.data.mlm_dtypes[\"ordinal\"]:\n",
    "    mlmachine_titanic.eda_cat_target_cat_feat(\n",
    "        feature=feature,\n",
    "        legend_labels=[\"Died\",\"Survived\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br><br>\n",
    "Star the [GitHub repository](https://github.com/petersontylerd/mlmachine), and stay tuned for additional notebooks.\n",
    "<br><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
